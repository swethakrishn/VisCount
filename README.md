# VisCount: Evaluating Counting Capabilities of Multimodal Video-LLMs

**Abstract:**
> *The rise of multimodal Video-LLMs, driven by advancements in Large Language Models (LLMs), has enabled powerful capabilities in video understanding tasks such as spatio-temporal reasoning, narrative comprehension, and video-based Visual Question Answering (VQA). However, despite the development of evaluation benchmarks for Video-LLMs, most existing efforts focus on complex reasoning tasks, overlooking evaluation on  fundamental vision tasks like object counting. In this paper, we present VisCount, a novel dataset that comprehensively assesses the performance of multimodal Video-LLMs across low object frequencies. We evaluate 5 recent models, including both open-source and closed-source variants, and find that most of the Video-LLMs, especially open-source ones, struggle with counting accuracies over images and videos. Additionally, we find that performance of these models degrades with increasing object frequencies. Our findings provide valuable insights for robustness of video LLMs, revealing key performance gaps in video-based multimodal models.*

**Examples:**
![alt text](img/VisCount_ Dataset_example)
![alt text](img/VisCount_ Dataset_video3)

