<img src="img/dataset_visual.png" height="120px" align="left">

# VisCount

> **VisCount: Evaluating Counting Capabilities of Multimodal Video-LLMs**  
> Swetha Krishnan, Rahasya Barkur, Kushal Raju   
> COMPSCI 670, Fall 2024

VisCount is a comprehensive dataset for more robust evaluations of multimodal video-LLMs, containing images and videos with varying object frequencies and object classes. VisCount aims to bridge the gap in evaluating the object counting capabilities of MultiModal-LLMs.


<h5 align="center"> If you like our project, please give us a star ‚≠ê on GitHub for the latest update.</h5>

**Abstract:**
> *The rise of multimodal Video-LLMs, driven by advancements in Large Language Models (LLMs), has enabled powerful capabilities in video understanding tasks such as spatio-temporal reasoning, narrative comprehension, and video-based Visual Question Answering (VQA). However, despite the development of evaluation benchmarks for Video-LLMs, most existing efforts focus on complex reasoning tasks, overlooking evaluation on  fundamental vision tasks like object counting. In this paper, we present VisCount, a novel dataset that comprehensively assesses the performance of multimodal Video-LLMs across low object frequencies. We evaluate 5 recent models, including both open-source and closed-source variants, and find that most of the Video-LLMs, especially open-source ones, struggle with counting accuracies over images and videos. Additionally, we find that performance of these models degrades with increasing object frequencies. Our findings provide valuable insights for robustness of video LLMs, revealing key performance gaps in video-based multimodal models.*

**Examples:**

Image
![VisCount_img](/img/VisCount_Dataset_example.png)
---


https://github.com/user-attachments/assets/006b2228-367c-4cf0-a72e-9820f6752736


![VisCount_video](/img/VisCount_Dataset_video3.png)

